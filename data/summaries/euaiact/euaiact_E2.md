# E2: Risk Classification

The EU AI Act uses a **risk-based approach** to regulate AI systems. Requirements are proportional to the level of risk an AI system poses to health, safety, and fundamental rights.

## The Four Risk Tiers

### 1. Unacceptable Risk (Prohibited)
AI systems that pose an unacceptable risk are **banned entirely**. See section E3 for details.

### 2. High-Risk
AI systems that can significantly impact health, safety, or fundamental rights. Subject to **strict requirements** before market placement. See section E4 for requirements.

### 3. Limited Risk
AI systems with **transparency obligations** — users must be informed they're interacting with AI. See section E5 for details.

### 4. Minimal Risk
All other AI systems — **no specific requirements** under the Act (e.g., spam filters, AI in video games).

---

## High-Risk AI Categories

High-risk AI systems fall into two main categories:

### Category 1: Safety Components & Regulated Products

AI systems that are:
- **Safety components** of products covered by EU harmonization legislation, OR
- Products themselves that require **third-party conformity assessment**

Covered product legislation includes:
- Machinery Regulation
- Toy Safety Directive
- Medical Devices Regulation
- In Vitro Diagnostic Devices Regulation
- Civil Aviation Regulation
- Motor Vehicle Type-Approval Regulation
- Rail System Interoperability Directive

### Category 2: Annex III Listed Systems

AI systems used in these areas (unless exceptions apply):

| Area | Examples |
|------|----------|
| **Biometrics** | Remote biometric identification, emotion recognition in workplace/education |
| **Critical Infrastructure** | Safety components in water, gas, electricity, transport management |
| **Education & Training** | Admission decisions, learning assessment, proctoring |
| **Employment** | Recruitment, promotion, termination decisions, task allocation, performance monitoring |
| **Essential Services** | Credit scoring, insurance pricing, emergency services dispatch |
| **Law Enforcement** | Risk assessment, polygraphs, evidence evaluation, profiling |
| **Migration & Border** | Visa applications, asylum claims, border surveillance |
| **Justice & Democracy** | Judicial decision support, election influence |

## High-Risk Exceptions

An Annex III AI system is **not** considered high-risk if it:

1. Performs a **narrow procedural task**, OR
2. **Improves the result** of a previously completed human activity, OR
3. Detects **decision-making patterns** without replacing human assessment, OR
4. Performs a **preparatory task** for an assessment

**However**, the exception does **not** apply if the AI system performs **profiling of natural persons**.

## Provider's Self-Assessment

Providers must document their risk classification reasoning. If a provider considers their Annex III system not high-risk, they must:

1. Document the assessment **before** placing on market
2. Provide documentation to national authorities **on request**
3. **Register** the system in the EU database

## Commission Updates

The European Commission can update the list of high-risk AI systems through delegated acts, considering:
- Severity of potential harm
- Probability of harm occurring
- Reversibility of harm
- Number of people affected
- Extent of dependency on AI output
- Vulnerability of affected groups
- Imbalance of power between deployer and affected persons
